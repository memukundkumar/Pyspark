{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b68b288-0f34-4377-aa6b-33b9d820e73b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "data = [\n",
    "    [\"C00295\", \"Rajesh\", \"Kapur\", 38, \"Artist\"],\n",
    "    [\"C00296\", \"Ben\", \"Smith\", 39, \"Banker\"],\n",
    "    [\"C00297\", \"Rohn\", \"Couper\", 30, \"Painter\"],\n",
    "    [\"C00298\", \"Rohn\", \"Couper\", 30, \"Painter\"],\n",
    "    [\"C00299\", \"John\", \"Doe\", 45, \"Engineer\"],\n",
    "    [\"C00299\", \"Roman\", \"Adam\", 45, \"Accountant\"],\n",
    "    [\"C00299\", \"Robert\", \"Allen\", 45, \"Teacher\"],\n",
    "    [\"C00299\", \"Rowan\", \"Anderson\", 45, \"Driver\"],\n",
    "    [\"C00299\", \"River\", \"Baker\", 45, \"Driver\"]\n",
    "\n",
    " ]\n",
    "\n",
    "#Prints Number of records\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "filtered_rdd = rdd.filter(lambda name: name[1].startswith(\"R\"))\n",
    "\n",
    "count = filtered_rdd.count()\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a8f9722-93e5-4f13-b6c9-46450e2e910e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rajesh\nRohn\nRohn\nRoman\nRobert\nRowan\nRiver\n"
     ]
    }
   ],
   "source": [
    "#Prints all name starting with R\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "filtered_rdd = rdd.filter(lambda name: name[1].startswith(\"R\"))\n",
    "\n",
    "# Collect the filtered RDD into a list\n",
    "collected_rdd = filtered_rdd.collect()\n",
    "\n",
    "# Print each name starting with R\n",
    "for name in collected_rdd:\n",
    "    print(name[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49931eb2-23dd-4cff-b5b6-18a885cf7288",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\r\n  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\r\nInstalling collected packages: tabulate\r\nSuccessfully installed tabulate-0.9.0\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-6fdf71db-c2a7-46ea-aff2-58fbb7888afc/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eae95c6-b051-4dba-9732-10844903e04c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustID    Fname    Lname       Age  Profession\n--------  -------  --------  -----  ------------\nC00295    Rajesh   Kapur        38  Artist\nC00297    Rohn     Couper       30  Painter\nC00298    Rohn     Couper       30  Painter\nC00299    Roman    Adam         45  Accountant\nC00299    Robert   Allen        45  Teacher\nC00299    Rowan    Anderson     45  Driver\nC00299    River    Baker        45  Driver\n"
     ]
    }
   ],
   "source": [
    "#Prints entire row where name starts with R\n",
    "import tabulate\n",
    "# Print the entire row\n",
    "#rdd = sc.parallelize(data)\n",
    "filtered_rdd = rdd.filter(lambda name: name[1].startswith(\"R\"))\n",
    "headers = [\"CustID\", \"Fname\", \"Lname\", \"Age\", \"Profession\"]\n",
    "# Print the filtered data rows in a tabular format\n",
    "print(tabulate.tabulate(filtered_rdd.collect(), headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce361ad3-4962-43e1-a903-a1f2582f39ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Joining and Preparing DataFrames for Student and Marks Data in PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"StudentMarksJoin\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ec2ba7c-fa08-4290-b867-38c96581f8d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Student DataFrame from data\n",
    "student_data = [\n",
    "    (1, \"John\", \"Smith\", \"Male\", 18),\n",
    "    (2, \"Emma\", \"Johnson\", \"Female\", 19),\n",
    "    (3, \"Michael\", \"Davis\", \"Male\", 20),\n",
    "    (4, \"Sophia\", \"Brown\", \"Female\", 18),\n",
    "    (5, \"Daniel\", \"Miller\", \"Male\", 19),\n",
    "]\n",
    "\n",
    "student_df = spark.createDataFrame(student_data,\n",
    "                                  schema=[\"StudentID\", \"FirstName\", \"LastName\", \"Gender\", \"Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9270e82-0f75-405d-a960-2f390f889a15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Marks DataFrame from data\n",
    "marks_data = [\n",
    "    (1, \"Math\", 85),\n",
    "    (2, \"Science\", 92),\n",
    "    (3, \"English\", 78),\n",
    "    (4, \"Math\", 89),\n",
    "    (5, \"Science\", 75),\n",
    "    (6, \"English\", 94),\n",
    "    (7, \"Math\", 80),\n",
    "    (8, \"Science\", 88),\n",
    "]\n",
    "\n",
    "marks_df = spark.createDataFrame(marks_data, schema=[\"StudentID\", \"Subject\", \"Marks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cfc3655-69e4-49ce-972d-cc397cf80599",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+------+---+-------+-----+\n|StudentID|FirstName|LastName|Gender|Age|Subject|Marks|\n+---------+---------+--------+------+---+-------+-----+\n|        1|     John|   Smith|  Male| 18|   Math|   85|\n|        2|     Emma| Johnson|Female| 19|Science|   92|\n|        3|  Michael|   Davis|  Male| 20|English|   78|\n|        4|   Sophia|   Brown|Female| 18|   Math|   89|\n|        5|   Daniel|  Miller|  Male| 19|Science|   75|\n+---------+---------+--------+------+---+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Join DataFrames:\n",
    "# Inner Join: Students with their marks\n",
    "joined_df = student_df.join(marks_df, \"StudentID\", \"inner\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8204b19-5569-4bbc-971f-085fcedd375e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+-------+-----+\n|StudentID|FirstName|LastName|Subject|Marks|\n+---------+---------+--------+-------+-----+\n|        1|     John|   Smith|   Math|   85|\n|        2|     Emma| Johnson|Science|   92|\n|        3|  Michael|   Davis|English|   78|\n|        4|   Sophia|   Brown|   Math|   89|\n|        5|   Daniel|  Miller|Science|   75|\n+---------+---------+--------+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Select Specific Columns\n",
    "filtered_df = joined_df.select(\"StudentID\", \"FirstName\", \"LastName\", \"Subject\", \"Marks\")\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd3346ef-fb8e-4908-98bf-fd57671dfcda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+------+---+-------+-----+\n|StudentID|FirstName|LastName|Gender|Age|Subject|Marks|\n+---------+---------+--------+------+---+-------+-----+\n|        1|     John|   Smith|  Male| 18|   Math|   85|\n|        4|   Sophia|   Brown|Female| 18|   Math|   89|\n+---------+---------+--------+------+---+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Filter Rows Based on Conditions\n",
    "filtered_df = joined_df.filter((joined_df[\"Marks\"] > 80) & (joined_df[\"Subject\"] == \"Math\"))\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c63c71ae-4b51-4b6d-ab44-7fc5019f5c5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+------+---+------------+\n|StudentID|FirstName|LastName|Gender|Age|AverageMarks|\n+---------+---------+--------+------+---+------------+\n|        1|     John|   Smith|  Male| 18|        85.0|\n|        2|     Emma| Johnson|Female| 19|        92.0|\n|        3|  Michael|   Davis|  Male| 20|        78.0|\n|        4|   Sophia|   Brown|Female| 18|        89.0|\n|        5|   Daniel|  Miller|  Male| 19|        75.0|\n+---------+---------+--------+------+---+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average marks for each student\n",
    "from pyspark.sql.functions import col, avg  # Import avg function\n",
    "average_marks_df = joined_df.groupBy(\"StudentID\", \"FirstName\", \"LastName\", \"Gender\", \"Age\") \\\n",
    "                            .agg(avg(\"Marks\").alias(\"AverageMarks\"))\n",
    "\n",
    "# Show the result\n",
    "average_marks_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d84b2d8-2b93-45e9-b859-2a3ed91944d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+---+--------------------+\n|  1|Kristina|  Chung| 55|               Pilot|\n+---+--------+-------+---+--------------------+\n|  2|   Paige|   Chen| 74|             Teacher|\n|  3|  Sherri| Melton| 34|         Firefighter|\n|  4|Gretchen|   Hill| 66|Computer hardware...|\n|  5|   Karen|Puckett| 74|              Lawyer|\n|  6| Patrick|   Song| 42|        Veterinarian|\n+---+--------+-------+---+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Reading A text file in the dataframe\n",
    "# Import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"ReadCustsFile\").getOrCreate()\n",
    "\n",
    "# Define file path\n",
    "file_path = \"/FileStore/tables/custs.txt\"\n",
    "\n",
    "# Read the file as a DataFrame\n",
    "custs_df = spark.read.csv(file_path, header=True)\n",
    "\n",
    "# Show the first 5 rows\n",
    "custs_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark RDD",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
